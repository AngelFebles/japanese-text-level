{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"japanese_text_level","text":"<p>A CLI tool for analyzing Japanese text and determining the WaniKani level required to read specific percentages (80%, 90%, 95%, and 100%) of the kanji and vocabulary present.</p> <p>Full documentation available here.</p>"},{"location":"#installation","title":"Installation","text":"<p>This project is designed to be used as a global CLI tool.</p> <p>Install using pipx:</p> <pre><code>pipx install \"git+https://github.com/AngelFebles/japanese-text-level\"\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<p>Analyze a .txt file containing Japanese text:</p> <pre><code>wk_level path/to/text.txt\n</code></pre> <p>Or use the bundled example text:</p> <pre><code>wk_level --example\n</code></pre> <p>Example output:</p> <pre><code>--- Example Text ---\n\u4eca\u65e5\u3001\u3044\u3044\u5929\u6c17\u3067\u3059\u306d\u301c\n--------------------\n\nWanikani levels to read kanji: {'80%': 3, '90%': 3, '95%': 3, '100%': 3}\nWanikani levels to read vocab: {'80%': 4, '90%': 4, '95%': 4, '100%': 4}\n\n</code></pre>"},{"location":"#development","title":"Development","text":""},{"location":"#instalation","title":"Instalation","text":"<p>To work on the project locally, first clone project:</p> <pre><code>git clone https://github.com/AngelFebles/japanese-text-level\ncd japanese-text-level\n</code></pre> <p>and then either install with pip:</p> <pre><code>python -m venv .venv\nsource .venv/bin/activate\npip install -e \".[dev]\"\n</code></pre> <p>or, if using uv, simply run:</p> <pre><code>uv sync\n</code></pre>"},{"location":"#running-quality-checks","title":"Running Quality Checks","text":"<p>Unit tests:</p> <pre><code>pytest\n</code></pre> <p>Linting:</p> <pre><code>ruff check\n</code></pre> <p>Formatting:</p> <pre><code>ruff format\n</code></pre> <p>Type checking:</p> <pre><code>ty check\n</code></pre> <p>Run all checks in one line:</p> <pre><code>pytest &amp;&amp; ruff check &amp;&amp; ruff format &amp;&amp; ty check\n</code></pre>"},{"location":"main/","title":"Main","text":"<p>japanese_text_level</p> <p>A CLI tool for analyzing Japanese text and determining the WaniKani level required to comprehend specific percentages of the kanji and vocabulary present in the text.</p> <p>The tool supports analyzing a user-provided .txt file or a bundled example text for demonstration purposes.</p>"},{"location":"main/#japanese_text_level.main.get_kanji_wanikani_levels","title":"<code>get_kanji_wanikani_levels(raw_text, wanikani_kanji)</code>","text":"<p>Returns mapping of WaniKani level needed to read difference percentages of the kanji in raw_text.</p> <p>Args:</p> <pre><code>raw_text (str): Raw input text from which kanji will be extracted.\n\nwanikani_kanji (dict[str, int]): Mapping of kanji \u2192 level.\n</code></pre> <p>Returns:</p> <pre><code>dict: mapping of the levels needed to read different percentaged of the raw text\n      { percentage \u2192 level }\n</code></pre> Source code in <code>src/japanese_text_level/main.py</code> <pre><code>def get_kanji_wanikani_levels(raw_text: str, wanikani_kanji: dict) -&gt; dict:\n    \"\"\"\n    Returns mapping of WaniKani level needed to read difference percentages of the kanji in raw_text.\n\n    Args:\n\n        raw_text (str): Raw input text from which kanji will be extracted.\n\n        wanikani_kanji (dict[str, int]): Mapping of kanji \u2192 level.\n\n    Returns:\n\n        dict: mapping of the levels needed to read different percentaged of the raw text\n              { percentage \u2192 level }\n\n    \"\"\"\n\n    # This solution for filtering with regex (Script=Han) doesn't cover 100% of kanjis,\n    # it excludes some obscure/historical/incredibly rare characters.\n    # ex: \u3006 (Unicode: U+3006) or \ud85a\uded6 (Unicode: U+26AD6)\n    # It is still good, however, for ~99.9% of cases.\n\n    # Since none of the omitted characters are present\n    # in Wanikani/JLPT/J\u014dy\u014d kanji lists (the scope of this project)\n    # I implemented this solution.\n    # For a 100% one, refer to: https://ayaka.shn.hk/hanregex/\n\n    kanjis_text = re.findall(r\"\\p{Script=Han}\", raw_text)\n\n    kanji_levels = [wanikani_kanji.get(item, 61) for item in kanjis_text]\n\n    # print(kanji_levels)\n\n    # print(kanji_levels)\n\n    if kanji_levels == []:\n        return {}\n    else:\n        return {\n            \"80%\": int(np.percentile(kanji_levels, 80)),\n            \"90%\": int(np.percentile(kanji_levels, 90)),\n            \"95%\": int(np.percentile(kanji_levels, 95)),\n            \"100%\": max(kanji_levels),\n        }\n</code></pre>"},{"location":"main/#japanese_text_level.main.get_vocab_wanikani_levels","title":"<code>get_vocab_wanikani_levels(raw_text, wanikani_vocab)</code>","text":"<p>Returns mapping of WaniKani level needed to read difference percentages of the vocab in raw_text.</p> <p>Args:</p> <pre><code>raw_text (str): Raw input text from which kanji will be extracted.\n\nwanikani_vocab (dict[str, int]): Mapping of vocab \u2192 level.\n</code></pre> <p>Returns:</p> <pre><code>dict: mapping of the levels needed to read different percentaged of the raw text\n      { percentage \u2192 level }\n</code></pre> Source code in <code>src/japanese_text_level/main.py</code> <pre><code>def get_vocab_wanikani_levels(raw_text: str, wanikani_vocab: dict) -&gt; dict:\n    \"\"\"\n     Returns mapping of WaniKani level needed to read difference percentages of the vocab in raw_text.\n\n    Args:\n\n        raw_text (str): Raw input text from which kanji will be extracted.\n\n        wanikani_vocab (dict[str, int]): Mapping of vocab \u2192 level.\n\n    Returns:\n\n        dict: mapping of the levels needed to read different percentaged of the raw text\n              { percentage \u2192 level }\n    \"\"\"\n\n    found_vocab = []\n\n    # Code runs well if we unify the pattern creation into a single expression\n    # but type checkers complain for [no-matching-overload] if you don't\n    # separate re.compile and the 'join' call\n\n    vocab_keys = list(wanikani_vocab.keys())\n    vocab_keys.sort(key=len, reverse=True)\n    escaped_vocab = [re.escape(word) for word in vocab_keys]\n\n    joined_pattern = \"|\".join(escaped_vocab)\n\n    pattern = re.compile(f\"(?=({joined_pattern}))\")\n\n    found_vocab = [m.group(1) for m in pattern.finditer(raw_text)]\n\n    vocab_levels = []\n\n    for item in found_vocab:\n        if item in wanikani_vocab:\n            vocab_levels.append(wanikani_vocab[item])\n        else:\n            vocab_levels.append(61)\n    if vocab_levels == []:\n        return {}\n    else:\n        return {\n            \"80%\": int(np.percentile(vocab_levels, 80)),\n            \"90%\": int(np.percentile(vocab_levels, 90)),\n            \"95%\": int(np.percentile(vocab_levels, 95)),\n            \"100%\": max(vocab_levels),\n        }\n</code></pre>"},{"location":"main/#japanese_text_level.main.get_wanikani_data","title":"<code>get_wanikani_data(wanikani_kanji_path, wanikani_vocab_path)</code>","text":"<p>Reads the WaniKani kanji and vocabulary JSON files and inverts them for fast lookups.</p> This function loads both JSON files into memory and converts them from <p>level \u2192 [item1, item2, ...]</p> <p>to:     item \u2192 level This avoids repeated scans for individual lookups.</p> <p>Args:</p> <pre><code>wanikani_kanji_path (Traversable): Path to the kanji JSON file by level.\n\nwanikani_vocab_path (Traversable): Path to the vocabulary JSON file by level.\n</code></pre> <p>Returns:</p> <pre><code>dict: A dictionary with two keys: \"kanji\" and \"vocab\", each mapping\n    items to their levels:\n        {\n            \"kanji\": dict[str, int],  # kanji character \u2192 level\n            \"vocab\": dict[str, int]   # vocabulary word \u2192 level\n        }\n</code></pre> Source code in <code>src/japanese_text_level/main.py</code> <pre><code>def get_wanikani_data(\n    wanikani_kanji_path: Traversable, wanikani_vocab_path: Traversable\n) -&gt; dict:\n    \"\"\"\n    Reads the WaniKani kanji and vocabulary JSON files and inverts them\n    for fast lookups.\n\n    This function loads both JSON files into memory and converts them from:\n        level \u2192 [item1, item2, ...]\n    to:\n        item \u2192 level\n    This avoids repeated scans for individual lookups.\n\n    Args:\n\n        wanikani_kanji_path (Traversable): Path to the kanji JSON file by level.\n\n        wanikani_vocab_path (Traversable): Path to the vocabulary JSON file by level.\n\n    Returns:\n\n        dict: A dictionary with two keys: \"kanji\" and \"vocab\", each mapping\n            items to their levels:\n                {\n                    \"kanji\": dict[str, int],  # kanji character \u2192 level\n                    \"vocab\": dict[str, int]   # vocabulary word \u2192 level\n                }\n    \"\"\"\n\n    wanikani = {\"kanji\": {}, \"vocab\": {}}\n\n    with wanikani_kanji_path.open(\"r\", encoding=\"utf-8\") as file:\n        temp = json.load(file)\n\n        for level in temp:\n            for kanji in temp[level]:\n                wanikani[\"kanji\"][kanji] = int(level)\n\n    with wanikani_vocab_path.open(\"r\", encoding=\"utf-8\") as file:\n        temp = json.load(file)\n\n        for level in temp:\n            for vocab in temp[level]:\n                wanikani[\"vocab\"][vocab] = int(level)\n\n    return wanikani\n</code></pre>"},{"location":"main/#japanese_text_level.main.run_analysis","title":"<code>run_analysis()</code>","text":"<p>CLI entrypoint for the japanese_text_level tool.</p> <p>Parses command-line arguments, loads WaniKani reference data, reads the target text (either from a provided file path or the bundled example), and prints the calculated difficulty levels for both kanji and vocabulary.</p> Source code in <code>src/japanese_text_level/main.py</code> <pre><code>def run_analysis():\n    \"\"\"\n     CLI entrypoint for the japanese_text_level tool.\n\n    Parses command-line arguments, loads WaniKani reference data,\n    reads the target text (either from a provided file path or the\n    bundled example), and prints the calculated difficulty levels\n    for both kanji and vocabulary.\n\n    \"\"\"\n\n    # The description for the command\n    # They appear with the -h / --help flags\n\n    parser = argparse.ArgumentParser(\n        description=\"Calculate WaniKani level required to read a Japanese text.\"\n    )\n\n    group = parser.add_mutually_exclusive_group(required=True)\n\n    group.add_argument(\n        \"file\",\n        nargs=\"?\",\n        help=\"Path to input .txt file\",\n    )\n\n    group.add_argument(\n        \"--example\",\n        action=\"store_true\",\n        help=\"Run analysis on bundled example text\",\n    )\n\n    args = parser.parse_args()\n\n    # This finds the directory where main.py actually lives\n    # without this the project kinda breaks when installed as a package\n    # BASE_DIR = Path(__file__).resolve().parent.parent.parent\n    data_dir = files(\"japanese_text_level\").joinpath(\"files\")\n\n    # Now build the paths relative to the project root\n    wanikani_kanji_path = data_dir / \"kanjis_wanikani_levels.json\"\n    wanikani_vocab_path = data_dir / \"vocabs_wanikani_levels.json\"\n\n    # wanikani_kanji_path = BASE_DIR / \"files\" / \"kanjis_wanikani_levels.json\"\n    # wanikani_vocab_path = BASE_DIR / \"files\" / \"vocabs_wanikani_levels.json\"\n\n    wanikani_data = get_wanikani_data(\n        wanikani_kanji_path,\n        wanikani_vocab_path,\n    )\n\n    if args.example:\n        example_path = data_dir / \"example_text.txt\"\n        raw_text = example_path.read_text(encoding=\"utf-8\")\n\n        print(\"\\n--- Example Text ---\")\n        print(raw_text)\n        print(\"--------------------\\n\")\n    else:\n        raw_text = Path(args.file).read_text(encoding=\"utf-8\")\n\n    kanji_levels = get_kanji_wanikani_levels(raw_text, wanikani_data[\"kanji\"])\n    vocab_levels = get_vocab_wanikani_levels(raw_text, wanikani_data[\"vocab\"])\n\n    print(\"Wanikani levels to read kanji:\", kanji_levels)\n    print(\"Wanikani levels to read vocab:\", vocab_levels)\n</code></pre>"}]}