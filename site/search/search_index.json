{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"japanese_text_level","text":"<p>A CLI tool for analyzing Japanese text and determining reading difficulty levels based on WaniKani (and future systems like JLPT). It calculates the level required to read specific percentages (80%, 90%, 95%, 100%) of kanji and vocabulary in your text.</p> <p>Full documentation available here.</p> <p>See ROADMAP.md for the full development plan.</p>"},{"location":"#installation","title":"Installation","text":"<p>This project is designed to be used as a global CLI tool.</p> <p>Install using pipx:</p> <pre><code>pipx install \"git+https://github.com/AngelFebles/japanese-text-level\"\n</code></pre>"},{"location":"#usage","title":"Usage","text":""},{"location":"#cli-synopsis","title":"CLI Synopsis","text":"<pre><code>jp-level &lt;system&gt; &lt;input-type&gt; [target | --example]\n</code></pre> Component Options Description SYSTEM wk WaniKani (Kanji/Vocab levels 1-60) jlpt Coming Soon (N5 to N1) INPUT_TYPE text Standard .txt file processing url Planned (Direct website scraping) TARGET path/to/file Local path to your Japanese text --example Runs the bundled sample text"},{"location":"#examples","title":"Examples","text":"<p>Analyze a local file:</p> <pre><code>jp-level wk text path/to/file.txt\n</code></pre> <p>Or the bundled example:</p> <pre><code>jp-level wk text --example\n</code></pre> <p>Which outputs:</p> <pre><code>Example text:\n\u4eca\u65e5\u3001\u3044\u3044\u5929\u6c17\u3067\u3059\u306d\u301c\n\n\n------- WaniKani Analysis ------\nPercentage | LV Kanji | LV Vocab\n--------------------------------\n80%        | 3     | 4\n90%        | 3     | 4\n95%        | 3     | 4\n100%       | 4     | 4\n--------------------------------\n</code></pre>"},{"location":"#development","title":"Development","text":""},{"location":"#instalation","title":"Instalation","text":"<p>To work on the project locally, first clone project:</p> <pre><code>git clone https://github.com/AngelFebles/japanese-text-level\ncd japanese-text-level\n</code></pre> <p>and then either install with pip:</p> <pre><code>python -m venv .venv\nsource .venv/bin/activate\npip install -e \".[dev]\"\n</code></pre> <p>or, if using uv, simply run:</p> <pre><code>uv sync\n\n</code></pre>"},{"location":"#quality-checks","title":"Quality Checks","text":"<pre><code>pytest        # Unit tests\nruff check .  # Linting\nruff format . # Formatting\nty check .    # Type checking\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<p>Documentation website is handled with mkdocs. To preview localy:</p> <pre><code>mkdocs serve\n</code></pre>"},{"location":"#project-structure","title":"Project Structure","text":"<ul> <li>src/japanese_text_level/<ul> <li>main.py - CLI Entrypoint and Argument Parsing.</li> <li>systems/ - Analysis logic for different grading systems (e.g., wk.py).</li> <li>files/ - Reference JSON datasets and example files.</li> </ul> </li> <li>test/ - Automated test suite.</li> </ul>"},{"location":"ROADMAP/","title":"Future Features / Ideas","text":""},{"location":"ROADMAP/#v10-cli-tool-core-analysis-logic","title":"v1.0 - CLI Tool &amp; Core Analysis Logic","text":"<ul> <li>[x] Wanikani (Levels 1-60; Kanji &amp; Vocab)</li> <li>[ ] JLTP (N5-N1) possible source.</li> <li>[ ] J\u014dyo Kanji</li> <li>[ ] Webscrapping text from link.</li> </ul>"},{"location":"ROADMAP/#v20-web-interface","title":"v2.0 - Web interface","text":"<ul> <li>[ ] React/Vue interface.</li> <li>[ ] FastAPI Wrapper.</li> <li>[ ] Containerized environment for cloud deployment with Docker.</li> </ul>"},{"location":"ROADMAP/#v30-support-for-more-formats","title":"v3.0 - Support for more formats","text":"<ul> <li>[ ] CLI Image recognition. Two options:<ul> <li>[ ] Pytesseract for OCR.</li> <li>[ ] Pre-trained computer vision model like Yolov5. Probably only possible in locally hosted version of the tool.</li> </ul> </li> <li>[ ] Pdf</li> <li>[ ] .srt (subtitle files)</li> </ul>"},{"location":"main/","title":"Main","text":"<p>japanese_text_level</p> <p>A CLI tool for analyzing Japanese text and determining the WaniKani level required to comprehend specific percentages of the kanji and vocabulary present in the text.</p> <p>The tool supports analyzing a user-provided .txt file or a bundled example text for demonstration purposes.</p> USAGE <p>jp-level system input-type target</p> <pre><code>SYSTEMS:\n    wk          Use WaniKani level standards (Current)\n    jlpt        Use JLPT level standards (Coming Soon)\n\nINPUT-TYPES:\n    text        Analyze raw .txt files\n\nTARGETS:\n    file        Path to a valid UTF-8 encoded text file\n    --example   Use the internal example_text.txt\n</code></pre>"},{"location":"main/#japanese_text_level.main.run_analysis","title":"<code>run_analysis()</code>","text":"<p>CLI entrypoint for japanese_text_level.</p> <p>This module defines the command-line interface and dispatches to the appropriate analysis system (e.g., WaniKani, JLPT).</p> <p>The CLI follows the structure:</p> <pre><code>jp-level system input-type input\n</code></pre> Example <p>jp-level wk text file.txt</p> <p>jp-level wk text --example</p> Source code in <code>src/japanese_text_level/main.py</code> <pre><code>def run_analysis():\n    \"\"\"\n    CLI entrypoint for japanese_text_level.\n\n    This module defines the command-line interface and dispatches\n    to the appropriate analysis system (e.g., WaniKani, JLPT).\n\n    The CLI follows the structure:\n\n        jp-level system input-type input\n\n    Example:\n        jp-level wk text file.txt\n\n        jp-level wk text --example\n    \"\"\"\n\n    parser = argparse.ArgumentParser(\n        prog=\"jp-level\",\n        description=\"Japanese text difficulty analysis tool.\",\n    )\n\n    subparsers = parser.add_subparsers(dest=\"system\", required=True)\n\n    # ---- WK system ----\n    wk_parser = subparsers.add_parser(\"wk\", help=\"WaniKani-based analysis\")\n    wk_subparsers = wk_parser.add_subparsers(dest=\"input_type\", required=True)\n\n    wk_text_parser = wk_subparsers.add_parser(\"text\", help=\"Analyze text file\")\n\n    # Mutually exclusive group: either provide a file OR --example\n    group = wk_text_parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\n        \"file\",\n        nargs=\"?\",\n        help=\"Path to input .txt file\",\n    )\n    group.add_argument(\n        \"--example\",\n        action=\"store_true\",\n        help=\"Run analysis on bundled example text\",\n    )\n\n    args = parser.parse_args()\n\n    if args.system == \"wk\" and args.input_type == \"text\":\n        # Determine source of text\n        if args.example:\n            data_dir = files(\"japanese_text_level\").joinpath(\"files\")\n            example_path = data_dir / \"example_text.txt\"\n            raw_text = example_path.read_text(encoding=\"utf-8\")\n            print(\"Example text:\")\n            print(raw_text)\n        else:\n            raw_text = Path(args.file).read_text(encoding=\"utf-8\")\n\n        # Analyze text\n        kanji_levels, vocab_levels = analyze_text(raw_text)\n\n        # Print results as a table\n        print(\"\\n------- WaniKani Analysis ------\")\n        print(f\"{'Percentage':&lt;10} | {'LV Kanji':&lt;7} | {'LV Vocab':&lt;7}\")\n        print(\"-\" * 32)\n\n        # Iterate through the percentages, sorted numerically\n        for pct in sorted(kanji_levels.keys(), key=lambda x: int(x.rstrip(\"%\"))):\n            print(f\"{pct:&lt;10} | {kanji_levels[pct]:&lt;5} | {vocab_levels[pct]:&lt;5}\")\n\n        print(\"--------------------------------\\n\")\n</code></pre>"},{"location":"wk/","title":"Wanikani","text":"<p>WaniKani analysis system.</p> <p>Provides functions to determine the WaniKani level required to comprehend specified percentages of kanji and vocabulary within a given Japanese text.</p>"},{"location":"wk/#japanese_text_level.systems.wk.get_kanji_wanikani_levels","title":"<code>get_kanji_wanikani_levels(raw_text, wanikani_kanji)</code>","text":"<p>Returns mapping of WaniKani level needed to read difference percentages of the kanji in raw_text.</p> <p>Note:</p> <p>This solution for filtering with regex (Script=Han) doesn't cover 100% of kanjis, it excludes some obscure/historical/incredibly rare characters. ex: \u3006 (Unicode: U+3006) or \ud85a\uded6 (Unicode: U+26AD6) It is still good, however, for ~99.9% of cases.</p> <p>Since none of the omitted characters are present in Wanikani/JLPT/J\u014dy\u014d kanji lists (the scope of this project) I implemented this solution. For a 100% one, refer to: https://ayaka.shn.hk/hanregex/</p> <p>Args:</p> <pre><code>raw_text (str): Raw input text from which kanji will be extracted.\n\nwanikani_kanji (dict[str, int]): Mapping of kanji \u2192 level.\n</code></pre> <p>Returns:</p> <pre><code>dict: mapping of the levels needed to read different percentaged of the raw text\n      { percentage \u2192 level }\n</code></pre> Source code in <code>src/japanese_text_level/systems/wk.py</code> <pre><code>def get_kanji_wanikani_levels(raw_text: str, wanikani_kanji: dict) -&gt; dict:\n    \"\"\"\n    Returns mapping of WaniKani level needed to read difference percentages of the kanji in raw_text.\n\n    Note:\n\n    This solution for filtering with regex (Script=Han) doesn't cover 100% of kanjis,\n    it excludes some obscure/historical/incredibly rare characters.\n    ex: \u3006 (Unicode: U+3006) or \ud85a\uded6 (Unicode: U+26AD6)\n    It is still good, however, for ~99.9% of cases.\n\n    Since none of the omitted characters are present\n    in Wanikani/JLPT/J\u014dy\u014d kanji lists (the scope of this project)\n    I implemented this solution.\n    For a 100% one, refer to: https://ayaka.shn.hk/hanregex/\n\n    Args:\n\n        raw_text (str): Raw input text from which kanji will be extracted.\n\n        wanikani_kanji (dict[str, int]): Mapping of kanji \u2192 level.\n\n    Returns:\n\n        dict: mapping of the levels needed to read different percentaged of the raw text\n              { percentage \u2192 level }\n\n    \"\"\"\n\n    # This solution for filtering with regex (Script=Han) doesn't cover 100% of kanjis,\n    # it excludes some obscure/historical/incredibly rare characters.\n    # ex: \u3006 (Unicode: U+3006) or \ud85a\uded6 (Unicode: U+26AD6)\n    # It is still good, however, for ~99.9% of cases.\n\n    # Since none of the omitted characters are present\n    # in Wanikani/JLPT/J\u014dy\u014d kanji lists (the scope of this project)\n    # I implemented this solution.\n    # For a 100% one, refer to: https://ayaka.shn.hk/hanregex/\n\n    kanjis_text = re.findall(r\"\\p{Script=Han}\", raw_text)\n\n    kanji_levels = [wanikani_kanji.get(item, 61) for item in kanjis_text]\n\n    # print(kanji_levels)\n\n    # print(kanji_levels)\n\n    if kanji_levels == []:\n        return {}\n    else:\n        return {\n            \"80%\": int(np.percentile(kanji_levels, 80)),\n            \"90%\": int(np.percentile(kanji_levels, 90)),\n            \"95%\": int(np.percentile(kanji_levels, 95)),\n            \"100%\": max(kanji_levels),\n        }\n</code></pre>"},{"location":"wk/#japanese_text_level.systems.wk.get_vocab_wanikani_levels","title":"<code>get_vocab_wanikani_levels(raw_text, wanikani_vocab)</code>","text":"<p>Returns mapping of WaniKani level needed to read difference percentages of the vocab in raw_text.</p> <p>Args:</p> <pre><code>raw_text (str): Raw input text from which kanji will be extracted.\n\nwanikani_vocab (dict[str, int]): Mapping of vocab \u2192 level.\n</code></pre> <p>Returns:</p> <pre><code>dict: mapping of the levels needed to read different percentaged of the raw text\n      { percentage \u2192 level }\n</code></pre> Source code in <code>src/japanese_text_level/systems/wk.py</code> <pre><code>def get_vocab_wanikani_levels(raw_text: str, wanikani_vocab: dict) -&gt; dict:\n    \"\"\"\n     Returns mapping of WaniKani level needed to read difference percentages of the vocab in raw_text.\n\n    Args:\n\n        raw_text (str): Raw input text from which kanji will be extracted.\n\n        wanikani_vocab (dict[str, int]): Mapping of vocab \u2192 level.\n\n    Returns:\n\n        dict: mapping of the levels needed to read different percentaged of the raw text\n              { percentage \u2192 level }\n    \"\"\"\n\n    # found_vocab = []\n\n    # Code runs well if we unify the pattern creation into a single expression\n    # but type checkers complain for [no-matching-overload] if you don't\n    # separate re.compile and the 'join' call\n\n    vocab_keys = list(wanikani_vocab.keys())\n    vocab_keys.sort(key=len, reverse=True)\n\n    escaped_vocab = [re.escape(word) for word in vocab_keys]\n    joined_pattern = \"|\".join(escaped_vocab)\n\n    pattern = re.compile(f\"(?=({joined_pattern}))\")\n    found_vocab = [m.group(1) for m in pattern.finditer(raw_text)]\n\n    vocab_levels = []\n\n    for item in found_vocab:\n        if item in wanikani_vocab:\n            vocab_levels.append(wanikani_vocab[item])\n        else:\n            vocab_levels.append(61)\n    if vocab_levels == []:\n        return {}\n    else:\n        return {\n            \"80%\": int(np.percentile(vocab_levels, 80)),\n            \"90%\": int(np.percentile(vocab_levels, 90)),\n            \"95%\": int(np.percentile(vocab_levels, 95)),\n            \"100%\": max(vocab_levels),\n        }\n</code></pre>"},{"location":"wk/#japanese_text_level.systems.wk.get_wanikani_data","title":"<code>get_wanikani_data(wanikani_kanji_path, wanikani_vocab_path)</code>","text":"<p>Reads the WaniKani kanji and vocabulary JSON files and inverts them for fast lookups.</p> This function loads both JSON files into memory and converts them from <p>level \u2192 [item1, item2, ...]</p> <p>to:     item \u2192 level This avoids repeated scans for individual lookups.</p> <p>Args:</p> <pre><code>wanikani_kanji_path (Traversable): Path to the kanji JSON file by level.\n\nwanikani_vocab_path (Traversable): Path to the vocabulary JSON file by level.\n</code></pre> <p>Returns:</p> <pre><code>dict: A dictionary with two keys: \"kanji\" and \"vocab\", each mapping\n    items to their levels:\n        {\n            \"kanji\": dict[str, int],  # kanji character \u2192 level\n            \"vocab\": dict[str, int]   # vocabulary word \u2192 level\n        }\n</code></pre> Source code in <code>src/japanese_text_level/systems/wk.py</code> <pre><code>def get_wanikani_data(\n    wanikani_kanji_path: Traversable, wanikani_vocab_path: Traversable\n) -&gt; dict:\n    \"\"\"\n    Reads the WaniKani kanji and vocabulary JSON files and inverts them\n    for fast lookups.\n\n    This function loads both JSON files into memory and converts them from:\n        level \u2192 [item1, item2, ...]\n    to:\n        item \u2192 level\n    This avoids repeated scans for individual lookups.\n\n    Args:\n\n        wanikani_kanji_path (Traversable): Path to the kanji JSON file by level.\n\n        wanikani_vocab_path (Traversable): Path to the vocabulary JSON file by level.\n\n    Returns:\n\n        dict: A dictionary with two keys: \"kanji\" and \"vocab\", each mapping\n            items to their levels:\n                {\n                    \"kanji\": dict[str, int],  # kanji character \u2192 level\n                    \"vocab\": dict[str, int]   # vocabulary word \u2192 level\n                }\n    \"\"\"\n\n    wanikani = {\"kanji\": {}, \"vocab\": {}}\n\n    with wanikani_kanji_path.open(\"r\", encoding=\"utf-8\") as file:\n        temp = json.load(file)\n\n        for level in temp:\n            for kanji in temp[level]:\n                wanikani[\"kanji\"][kanji] = int(level)\n\n    with wanikani_vocab_path.open(\"r\", encoding=\"utf-8\") as file:\n        temp = json.load(file)\n\n        for level in temp:\n            for vocab in temp[level]:\n                wanikani[\"vocab\"][vocab] = int(level)\n\n    return wanikani\n</code></pre>"}]}
